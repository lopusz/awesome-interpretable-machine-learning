* Awesome Interpretable Machine Learning [[https://awesome.re][https://awesome.re/badge.svg]]

Opinionated list of resources facilitating model interpretability
(introspection, simplification, visualization, explanation).

** Interpretable Models
   + Interpretable models
     + Simple decision trees
     + Rules
     + (Regularized) linear regression
     + k-NN

   + { "arxiv_id": "1511.01644" }

   + { "doi" : "10.1214/07-AOAS148" }

   + { "doi" : "10.1145/2594473.2594475" }
     + http://www.kdd.org/exploration_files/V15-01-01-Freitas.pdf
     + Interesting discussion of interpretability for a few  classification  models
       (decision trees, classification rules, decision tables, nearest neighbors  and  Bayesian  network  classifier)

   + { "arxiv_id" : "1711.04574" }

** Feature Importance
   + Models offering feature importance measures
     + Random forest
     + Boosted trees
     + Extremely randomized trees
       + { "doi" : "10.1007/s10994-006-6226-1" }
     + Random ferns
       + { "doi" : "10.18637/jss.v061.i10" }
         + https://cran.r-project.org/web/packages/rFerns
         + https://notabug.org/mbq/rFerns
     + Linear regression (with a grain of salt)

   + { "arxiv_id" : "1801.01489" }
     + https://github.com/aaronjfisher/mcr
     + Universal (model agnostic) variable importance measure

   + { "arxiv_id" : "1804.06620" }
     + https://github.com/giuseppec/featureImportance
     + Global and local (model agnostic) variable importance measure (based on Model Reliance)

   + { "doi" : "10.1186/1471-2105-8-25", "title" : "Bias in random forest variable importance measures: Illustrations, sources and a solution" }

   + { "doi" : "10.1186/1471-2105-9-307" }

   + Very good blog post describing deficiencies of random forest feature importance and the permutation importance
     + http://explained.ai/rf-importance/index.html

   + Permutation importance - simple model agnostic approach is described in Eli5 documentation
     + https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html

** Feature Selection
   + Classification of feature selection methods
     + Filters
     + Wrappers
     + Embedded methods

   + { "sems_id" :  "5264ae4ea4411426ddd91dc780c2892c3ff933d3", "skip_doi" : true }
     + http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf
     + Be sure to read this very illustrative introduction to feature selection

   + Filter Methods

     + { "arxiv_id" : "1711.08421" }

     + { "arxiv_id" : "1711.08477" }

     + { "sems_id" : "d72ff5063520ce4542d6d9b9e6a4f12aafab6091" }
       + https://pdfs.semanticscholar.org/d72f/f5063520ce4542d6d9b9e6a4f12aafab6091.pdf
       + Introduces information theoretic methods - double input symmetrical relevance (DISR)

     + { "sems_id" : "2629632a8032c02ad25021dbea82184077117d78" }
       + http://www.jmlr.org/papers/volume13/brown12a/brown12a.pdf
       + Code: https://github.com/Craigacp/FEAST
       + Discusses various approaches based on mutual information (MIM, mRMR, MIFS, CMIM, JMI, DISR, ICAP, CIFE, CMI)

     + { "sems_id" : "5a2f4bdab1499314a24e3a8d3c88dbfc6f6ecaf6" }
       + http://www.cs.man.ac.uk/~gbrown/publications/pocockPhDthesis.pdf

   + Wrapper methods

     + { "doi" : "10.18637/jss.v036.i11" }
       + https://cran.r-project.org/web/packages/Boruta/
       + Code (official, R): https://notabug.org/mbq/Boruta/
       + Code (Python): https://github.com/scikit-learn-contrib/boruta_py

     + Boruta for those in a hurry
       + https://cran.r-project.org/web/packages/Boruta/vignettes/inahurry.pdf

   + General

     + Special issue of JMLR of feature selection - oldish (2003)
       + http://www.jmlr.org/papers/special/feature03.html

     + { "sems_id" : "b14dea76cafede81c6ff5478d4221fce3aec9284" }
       + Paper: https://papers.nips.cc/paper/2728-result-analysis-of-the-nips-2003-feature-selection-challenge.pdf
       + Website http://clopinet.com/isabelle/Projects/NIPS2003/

     + { "sems_id" : "a93a1d9c9a1f46dc5aaadabedf906823d5043173" }
       + https://pdfs.semanticscholar.org/a83b/ddb34618cc68f1014ca12eef7f537825d104.pdf
       + Classic paper discussing weakly relevant features, irrelevant features, strongly relevant features

     + { "sems_id" : "00bc2153cd21001860e43758273945110f0ae40d" }
       + http://www.jmlr.org/papers/volume8/nilsson07a/nilsson07a.pdf
       + Discusses minimal optimal vs all-relevant approaches to feature selection

   + Feature Engineering and Selection by Kuhn & Johnson
     + Sligtly off-topic, but very interesting book
     + http://www.feat.engineering/index.html
     + https://bookdown.org/max/FES/
     + https://github.com/topepo/FES

  + Feature Engineering presentation by H. J. van Veen
    + Slightly off-topicm but very interesting deck of slides
    + Slides: https://www.slideshare.net/HJvanVeen/feature-engineering-72376750

** Model Explanations
*** Philosophy
    + Magnets by R. P. Feynman
      https://www.youtube.com/watch?v=wMFPe-DwULM

    + { "arxiv_id" : "1101.0891" }

    + { "arxiv_id" : "1606.03490" }
      + https://www.youtube.com/watch?v=mvzBQci04qA

    + { "arxiv_id" : "1711.07414" }

    + { "arxiv_id" : "1702.08608" }

    + [[http://bayes.cs.ucla.edu/WHY/why-intro.pdf][The Book of Why: The New Science of Cause and Effect]] by Judea Pearl

    + Looking Inside the Black Box, presentation of Leo Breiman
      + https://www.stat.berkeley.edu/users/breiman/wald2002-2.pdf

    + Please Stop Doing the "Explainable" ML by Cynthia Rudin
      + Video (starts 17:30, lasts 10 min): https://zoom.us/recording/play/0y-iI9HamgyDzzP2k_jiTu6jB7JgVVXnjWZKDMbnyRTn3FsxTDZy6Wkrj3_ekx4J
      + Linked at: https://users.cs.duke.edu/~cynthia/mediatalks.html

    + { "arxiv_id" : "1806.00069" }

    + { "arxiv_id" : "1901.04592" }

    + On Explainable Machine Learning Misconceptions A More Human-Centered Machine Learning by Patrick Hall
      + https://github.com/jphall663/xai_misconceptions/blob/master/xai_misconceptions.pdf
      + https://github.com/jphall663/xai_misconceptions

*** Model Agnostic Explanations
    + { "arxiv_id" : "0912.1128" }

    + { "arxiv_id" : "1309.6392" }

    + { "arxiv_id" : "1602.04938" }
      + Code: https://github.com/marcotcr/lime
      + https://github.com/marcotcr/lime-experiments
      + https://www.youtube.com/watch?v=bCgEP2zuYxI
      + Introduces the LIME method (Local Interpretable Model-agnostic Explanations)

    + { "arxiv_id" : "1606.09517" }
      + http://www.blackboxworkshop.org/pdf/Turner2015_MES.pdf

    + { "arxiv_id" : "1703.04730" }

    + { "arxiv_id" : "1705.07874" }
      + Code: https://github.com/slundberg/shap
      + Introduces the SHAP method (SHapley Additive exPlanations), generalizing LIME

    + { "sems_id" : "1c6690ab404b23d5026dd3ad0c7a49ce2875c1b3" }
      + https://homes.cs.washington.edu/~marcotcr/aaai18.pdf
      + Code: https://github.com/marcotcr/anchor-experiments

    + { "arxiv_id" : "1802.07814" }

    + { "arxiv_id" : "1804.01955" }
      + Docs: https://mi2datalab.github.io/live/
      + Code: https://github.com/MI2DataLab/live
      + Docs: https://pbiecek.github.io/breakDown
      + Code: https://github.com/pbiecek/breakDown

    + { "arxiv_id" : "1904.03867" }

    + A review book -  Interpretable Machine Learning. A Guide for Making Black Box
      Models Explainable by Christoph Molnar

      + https://christophm.github.io/interpretable-ml-book/

*** Model Specific Explanations - Neural Networks
    + { "arxiv_id" : "1311.2901" }

    + { "arxiv_id" : "1312.6034" }

    + { "arxiv_id" : "1506.06579" }
      + https://github.com/yosinski/deep-visualization-toolbox

    + { "arxiv_id" : "1610.02391" }

    + { "arxiv_id" : "1603.08507" }

    + { "arxiv_id" : "1606.04155" }
      + https://people.csail.mit.edu/taolei/papers/emnlp16_rationale_slides.pdf
      + Code: https://github.com/taolei87/rcnn/tree/master/code/rationale

    + { "arxiv_id" : "1611.02639" }

    + Pixel entropy can be used to detect relevant picture regions (for CovNets)
      + See Visualization section and Fig. 5 of the paper
        + { "arxiv_id" : "1703.07047" }

    + { "arxiv_id" : "1706.05806" }
      + https://research.googleblog.com/2017/11/interpreting-deep-neural-networks-with.html

    + { "arxiv_id" : "1712.06302" }

    + { "arxiv_id" : "1703.01365" }
      + Code: https://github.com/ankurtaly/Integrated-Gradients
      + Proposes Integrated Gradients Method
      + See also: Gradients of Counterfactuals https://arxiv.org/pdf/1611.02639.pdf

    + { "arxiv_id" : "1704.02685" }

      + Proposes Deep Lift method

      + Code: https://github.com/kundajelab/deeplift

      + Videos: https://www.youtube.com/playlist?list=PLJLjQOkqSRTP3cLB2cOOi_bQFw6KPGKML

    + { "arxiv_id" : "1711.0867" }
      + Review of failures for methods extracting most important pixels for prediction

    + { "arxiv_id" : "1805.08249" }

    + Classifier-agnostic Saliency Map Extraction
      + Code: https://github.com/kondiz/casme

    + The Building Blocks of Interpretability
      + https://distill.pub/2018/building-blocks
      + Has some embeded links to notebooks
      + Uses Lucid library https://github.com/tensorflow/lucid

    + { "arxiv_id" : "1808.04260" }
      + Code: https://github.com/albermax/innvestigate

    + { "arxiv_id" : "1811.02783" }

    + { "arxiv_id" : "1806.05337"}
      + Code: https://github.com/csinva/hierarchical_dnn_interpretations

** Extracting Interpretable Models From Complex Ones

   + { "arxiv_id" : "1711.09576" }

   + { "arxiv_id" : "1711.09784" }

   + { "sems_id" : "94770d1ea1192c05a6922712200689344a742d81" }
     + http://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_96.pdf

** Model Visualization
   + Visualizing Statistical Models: Removing the blindfold
     + http://had.co.nz/stat645/model-vis.pdf

   + Partial dependence plots
     + http://scikit-learn.org/stable/auto_examples/ensemble/plot_partial_dependence.html
     + pdp: An R Package for Constructing Partial Dependence Plots
       https://journal.r-project.org/archive/2017/RJ-2017-016/RJ-2017-016.pdf
       https://cran.r-project.org/web/packages/pdp/index.html

   + ggfortify: Unified Interface to Visualize Statistical Results of Popular R Packages
     + https://journal.r-project.org/archive/2016-2/tang-horikoshi-li.pdf
     + CRAN https://cran.r-project.org/web/packages/ggfortify/index.html

   + RandomForestExplainer
     + Master thesis https://rawgit.com/geneticsMiNIng/BlackBoxOpener/master/randomForestExplainer_Master_thesis.pdf
     + R code
       + CRAN https://cran.r-project.org/web/packages/randomForestExplainer/index.html
       + Code: https://github.com/MI2DataLab/randomForestExplainer

   + ggRandomForest
     + Paper (vignette) https://github.com/ehrlinger/ggRandomForests/raw/master/vignettes/randomForestSRC-Survival.pdf
     + R code
       + CRAN https://cran.r-project.org/web/packages/ggRandomForests/index.html
       + Code: https://github.com/ehrlinger/ggRandomForests

** Selected Review Talks and Tutorials
   + Tutorial on Interpretable machine learning at ICML 2017
     + Slides: http://people.csail.mit.edu/beenkim/papers/BeenK_FinaleDV_ICML2017_tutorial.pdf

   + P. Biecek, Show Me Your Model - Tools for Visualisation of Statistical Models
     + Video: https://channel9.msdn.com/Events/useR-international-R-User-conferences/useR-International-R-User-2017-Conference/Show-Me-Your-Model-tools-for-visualisation-of-statistical-models

   + S. Ritchie, Just-So Stories of AI
     + Video: https://www.youtube.com/watch?v=DiWkKqZChF0
     + Slides: https://speakerdeck.com/sritchie/just-so-stories-for-ai-explaining-black-box-predictions

   + C. Jarmul, Towards Interpretable Accountable Models
     + Video: https://www.youtube.com/watch?v=B3PtcF-6Dtc
     + Slides: https://docs.google.com/presentation/d/e/2PACX-1vR05kpagAbL5qo1QThxwu44TI5SQAws_UFVg3nUAmKp39uNG0xdBjcMA-VyEeqZRGGQtt0CS5h2DMTS/embed?start=false&loop=false&delayms=3000

   + I. Oszvald, Machine Learning Libraries You'd Wish You'd Known About
     + A large part of the talk covers model explanation and visualization
     + Video: https://www.youtube.com/watch?v=nDF7_8FOhpI
     + Associated notebook on explaining regression predictions: https://github.com/ianozsvald/data_science_delivered/blob/master/ml_explain_regression_prediction.ipynb

   + G. Varoquaux, Understanding and diagnosing your machine-learning models (covers PDP and Lime among others)
     + Video: https://www.youtube.com/watch?v=kbj3llSbaVA
     + Slides: http://gael-varoquaux.info/interpreting_ml_tuto/

** Venues
   + Interpretable ML Symposium (NIPS 2017) (contains links to *papers*, *slides* and *videos*)
     + http://interpretable.ml/
     + Debate, Interpretability is necessary in machine learning
       + https://www.youtube.com/watch?v=2hW05ZfsUUo
   + Workshop on Human Interpretability in Machine Learning (WHI), organised in conjunction with ICML
     + 2018 (contains links to *papers* and *slides*)
       + https://sites.google.com/view/whi2018
       + Proceedings https://arxiv.org/html/1807.01308
     + 2017 (contains links to *papers* and *slides*)
       + https://sites.google.com/view/whi2017/home
       + Proceedings https://arxiv.org/html/1708.02666
     + 2016 (contains links to *papers*)
       + https://sites.google.com/site/2016whi/
       + Proceedings https://arxiv.org/html/1607.02531 or [[https://drive.google.com/open?id=0B9mGJ4F63iKGZWk0cXZraTNjRVU][here]]
   + Analyzing and interpreting neural networks for NLP (BlackboxNLP), organised in conjunction with EMNLP
     + 2019 (links below may get prefixed by 2019 later on)
       + https://blackboxnlp.github.io/
       + https://blackboxnlp.github.io/program.html
       + Papers should be available on arXiv
     + 2018
       + https://blackboxnlp.github.io/2018
       + https://blackboxnlp.github.io/program.html
       + [[https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=BlackboxNLP&terms-0-field=comments&terms-1-operator=OR&terms-1-term=Analyzing+interpreting+neural+networks+NLP&terms-1-field=comments&classification-physics_archives=all&date-filter_by=all_dates&date-year=&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=200&order=-announced_date_first][List of papers]]
   + FAT/ML Fairness, Accountability, and Transparency in Machine Learning [[https://www.fatml.org/]]
     + 2018
       + https://www.fatml.org/schedule/2018
     + 2017
       + https://www.fatml.org/schedule/2017
     + 2016
       + https://www.fatml.org/schedule/2016
     + 2016
       + https://www.fatml.org/schedule/2016
     + 2015
       + https://www.fatml.org/schedule/2015
     + 2014
       + https://www.fatml.org/schedule/2014
    + AAAI/ACM Annual Conferenceon AI, Ethics, and Society
      + 2019 (links below may get prefixed by 2019 later on)
        + http://www.aies-conference.com/accepted-papers/
      + 2018
        + http://www.aies-conference.com/2018/accepted-papers/
        + http://www.aies-conference.com/2018/accepted-student-papers/
** Software
   Software related to papers is mentioned along with each publication.
   Here only standalone software is included.

   + DALEX - R package, Descriptive mAchine Learning EXplanations
     + CRAN https://cran.r-project.org/web/packages/DALEX/DALEX.pdf
     + Code: https://github.com/pbiecek/DALEX

   + ELI5 - Python package dedicated to debugging machine learning classifiers
     and explaining their predictions
     + Code: https://github.com/TeamHG-Memex/eli5
     + https://eli5.readthedocs.io/en/latest/

   + forestmodel - R package visualizing coefficients of different models with the so called forest plot
     + CRAN https://cran.r-project.org/web/packages/forestmodel/index.html
     + Code: https://github.com/NikNakk/forestmodel

   + fscaret - R package with automated Feature Selection from 'caret'
     + CRAN https://cran.r-project.org/web/packages/fscaret/
     + Tutorial: https://cran.r-project.org/web/packages/fscaret/vignettes/fscaret.pdf

   + iml - R package for Interpretable Machine Learning
     + CRAN https://cran.r-project.org/web/packages/iml/
     + Code: https://github.com/christophM/iml
     + Publication: http://joss.theoj.org/papers/10.21105/joss.00786

   + lime - R package implementing LIME
     + https://github.com/thomasp85/lime

   + lofo-importance - Python package feature importance by Leave One Feature Out Importance method
     + Code: https://github.com/aerdem4/lofo-importance

   + Lucid - a collection of infrastructure and tools for research in neural network interpretability
     + Code: https://github.com/tensorflow/lucid

   + praznik - R package with a collection of feature selection filters performing greedy optimisation of mutual information-based usefulness criteria, see JMLR 13, 27âˆ’66 (2012)
     + CRAN https://cran.r-project.org/web/packages/praznik/index.html
     + Code: https://notabug.org/mbq/praznik

   + yellowbrick - Python package offering visual analysis and diagnostic tools to facilitate machine learning model selection
     + Code: https://github.com/DistrictDataLabs/yellowbrick
     + http://www.scikit-yb.org/en/latest/

** Other Resources
   + *Awesome* list of resources
     + https://github.com/jphall663/awesome-machine-learning-interpretability
